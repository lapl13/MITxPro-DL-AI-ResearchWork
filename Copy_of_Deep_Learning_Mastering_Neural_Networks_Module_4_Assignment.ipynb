{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lapl13/MITxPro-DL-AI-ResearchWork/blob/main/Copy_of_Deep_Learning_Mastering_Neural_Networks_Module_4_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning: Mastering Neural Networks - Module 4 Asignment: CNN Classification of FashionMNIST Dataset\n",
        "\n",
        "Now that we have Convolutional layers in our deep learning toolkit, we are going tackle a new and more challenging dataset - the [FashionMNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist) dataset. This dataset is in a similar format to the MNIST dataset, however it contains images of different articles of clothing.\n",
        "\n",
        "In this assignment, we ask that you develop a CNN classifier that implements **at least 2** convolutional layers.\n",
        "\n",
        "A template has been provided below with some starter code and please feel free to reuse any code you have written or seen before in previous notebooks!\n",
        "\n"
      ],
      "metadata": {
        "id": "aZVJ8gUn1RR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.modules.flatten import Flatten\n",
        "import time, copy\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "# device config (train our model on GPU if it is available which is much faster)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "yrzZRul42zJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These transforms will be performed on every datapoint - in this example we want to transform every\n",
        "# datapoint to a Tensor datatype, and perform normalization\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
        "\n",
        "# Notice how FashionMNIST is also built into PyTorch!\n",
        "fashion_mnist_train = torchvision.datasets.FashionMNIST('', train=True, transform =transform, download=True)\n",
        "\n",
        "# We will split out train dataset into train and validation!\n",
        "fashion_mnist_train, fashion_mnist_val = torch.utils.data.random_split(fashion_mnist_train, [int(np.floor(len(fashion_mnist_train)*0.75)), int(np.ceil(len(fashion_mnist_train)*0.25))])\n",
        "\n",
        "fashion_mnist_test = torchvision.datasets.FashionMNIST('', train=False, transform = transform, download=True)"
      ],
      "metadata": {
        "id": "GHajaotB22E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will create DataLoaders just like before with a batch size of 100\n",
        "batch_size = 100\n",
        "dataloaders = {'train': DataLoader(fashion_mnist_train, batch_size=batch_size),\n",
        "               'val': DataLoader(fashion_mnist_val, batch_size=batch_size),\n",
        "               'test': DataLoader(fashion_mnist_test, shuffle=True, batch_size=batch_size)}\n",
        "\n",
        "dataset_sizes = {'train': len(fashion_mnist_train),\n",
        "                 'val': len(fashion_mnist_val),\n",
        "                 'test': len(fashion_mnist_test)}\n",
        "print(f'dataset_sizes = {dataset_sizes}')"
      ],
      "metadata": {
        "id": "h5YwZ-eb23vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization\n",
        "\n",
        "Before we go any further, we want to visualize the different datapoints in our dataset. Similar to the MNIST datset, there are 10 different object classes and they map to the following labels:\n",
        "\n",
        "* 0 - T-shirt/top\n",
        "* 1 - Trouser\n",
        "* 2 - Pullover\n",
        "* 3 - Dress\n",
        "* 4 - Coat\n",
        "* 5 - Sandal\n",
        "* 6 - Shirt\n",
        "* 7 - Sneaker\n",
        "* 8 - Bag\n",
        "* 9 - Ankle boot\n",
        "\n",
        "Using the function below, we can visualize some of the different images that we will be working with from each class. Additionally, follow this [link](https://www.kaggle.com/datasets/zalando-research/fashionmnist) to find more information on the dataset."
      ],
      "metadata": {
        "id": "j4Kk0EWBsKJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot a digit ground truth and autoencoding\n",
        "def view_fashionmnist(label, count = 1):\n",
        "    fig = plt.figure()\n",
        "    idx = 1\n",
        "    for inputs, labels in dataloaders[\"test\"]:\n",
        "        for i, input in enumerate(inputs):\n",
        "            # we only want to view a certain class\n",
        "            if (labels[i] != label):\n",
        "                continue\n",
        "            # plot the ground truth\n",
        "            ax = fig.add_subplot(1, count, idx)\n",
        "            input = input.cpu().detach().numpy().reshape((28,28))\n",
        "            ax.imshow(input, cmap='gray')\n",
        "            idx += 1\n",
        "            if idx > count:\n",
        "                break\n",
        "        if idx > count:\n",
        "            break"
      ],
      "metadata": {
        "id": "zPaxnQIstPRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View 6 bags\n",
        "view_fashionmnist(8, 6)"
      ],
      "metadata": {
        "id": "RZlW6xS3t5dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View 1 sneaker\n",
        "view_fashionmnist(7, 1)"
      ],
      "metadata": {
        "id": "Z2QOkZTLv7hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Turn\n",
        "\n",
        "Use what you have learned so far to develop a CNN classifier that implements **at least 2** convolutional layers."
      ],
      "metadata": {
        "id": "X-Mvvfm7weOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint! Create a CNNClassifier class that implements a forward function\n",
        "\n",
        "# class CNNClassifier(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(CNNClassifier, self).__init__()\n",
        "#         # Split the Encoder and Decoder\n",
        "\n",
        "#         self.pipeline =\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.pipeline(x)\n"
      ],
      "metadata": {
        "id": "kOXGrXkYvVss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# learning_rate =\n",
        "# num_epochs =\n",
        "# model ="
      ],
      "metadata": {
        "id": "Tl0UJSKjwBwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint! Try reusing one of the training functions we have previously written\n",
        "# def train_classification_model"
      ],
      "metadata": {
        "id": "R-IYOBzW4Kaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimizer\n",
        "# criterion =\n",
        "# optimizer =\n",
        "# scheduler =\n",
        "\n",
        "# Make sure you save the training curves along the way for visualization afterwards!\n",
        "# model, training_curves = train_classification_model(...)"
      ],
      "metadata": {
        "id": "XAiTgOnR4WfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Training Curves and Results"
      ],
      "metadata": {
        "id": "pVr2w17g2kVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions for plotting your results!\n",
        "def plot_training_curves(training_curves,\n",
        "                         phases=['train', 'val', 'test'],\n",
        "                         metrics=['loss','acc']):\n",
        "    epochs = list(range(len(training_curves['train_loss'])))\n",
        "    for metric in metrics:\n",
        "        plt.figure()\n",
        "        plt.title(f'Training curves - {metric}')\n",
        "        for phase in phases:\n",
        "            key = phase+'_'+metric\n",
        "            if key in training_curves:\n",
        "                if metric == 'acc':\n",
        "                    plt.plot(epochs, [item.detach().cpu() for item in training_curves[key]])\n",
        "                else:\n",
        "                    plt.plot(epochs, training_curves[key])\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(labels=phases)\n",
        "\n",
        "def classify_predictions(model, device, dataloader):\n",
        "    model.eval()   # Set model to evaluate mode\n",
        "    all_labels = torch.tensor([]).to(device)\n",
        "    all_scores = torch.tensor([]).to(device)\n",
        "    all_preds = torch.tensor([]).to(device)\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = torch.softmax(model(inputs),dim=1)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        scores = outputs[:,1]\n",
        "        all_labels = torch.cat((all_labels, labels), 0)\n",
        "        all_scores = torch.cat((all_scores, scores), 0)\n",
        "        all_preds = torch.cat((all_preds, preds), 0)\n",
        "    return all_preds.detach().cpu(), all_labels.detach().cpu(), all_scores.detach().cpu()\n",
        "\n",
        "def plot_cm(model, device, dataloaders, phase='test'):\n",
        "    class_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "    preds, labels, scores = classify_predictions(model, device, dataloaders[phase])\n",
        "\n",
        "    cm = metrics.confusion_matrix(labels, preds)\n",
        "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "    ax = disp.plot().ax_\n",
        "    ax.set_title('Confusion Matrix -- counts')\n"
      ],
      "metadata": {
        "id": "bFiFvHou4dOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_curves(training_curves, phases=['train', 'val', 'test'])"
      ],
      "metadata": {
        "id": "3MQF3p-74fQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = plot_cm(model, device, dataloaders, phase='test')"
      ],
      "metadata": {
        "id": "OwqcGBhv4f0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "Now that we have implemented our network we want to examine the results.\n",
        "\n",
        "What class of object is most often misclassified? What class is it incorrectly classified as most often?\n",
        "\n",
        "Additionally, try re-training your network with dropout included. Does this help the performance or is there a noticeable change in the ability of the model to generalize? Is the most commonly misclassified object still the same as with no dropout?"
      ],
      "metadata": {
        "id": "1pCcLS4Vw0-F"
      }
    }
  ]
}